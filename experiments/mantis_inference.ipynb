{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Users/lorenz/Repos/extractify/backend\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "WORKING_DIR = os.path.dirname(os.path.abspath('./'))\n",
    "print(WORKING_DIR)\n",
    "sys.path.append(WORKING_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv(\"../dataset/mantis_json_dataset_with_intents/mantis_utterance_category.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "current_model: AIModelType.GOOGLE_GEMINI_PRO\n",
      "raw response candidates {\n",
      "  content {\n",
      "    role: \"model\"\n",
      "    parts {\n",
      "      function_call {\n",
      "        name: \"entity_linking\"\n",
      "        args {\n",
      "          fields {\n",
      "            key: \"category\"\n",
      "            value {\n",
      "              string_value: \"dba\"\n",
      "            }\n",
      "          }\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "  avg_logprobs: -0.00019140884978696704\n",
      "  finish_reason: STOP\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HATE_SPEECH\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.0966796875\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.115722656\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_DANGEROUS_CONTENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.486328125\n",
      "    severity: HARM_SEVERITY_LOW\n",
      "    severity_score: 0.38671875\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_HARASSMENT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.122558594\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0673828125\n",
      "  }\n",
      "  safety_ratings {\n",
      "    category: HARM_CATEGORY_SEXUALLY_EXPLICIT\n",
      "    probability: NEGLIGIBLE\n",
      "    probability_score: 0.221679688\n",
      "    severity: HARM_SEVERITY_NEGLIGIBLE\n",
      "    severity_score: 0.0654296875\n",
      "  }\n",
      "}\n",
      "model_version: \"gemini-1.5-pro-001\"\n",
      "usage_metadata {\n",
      "  prompt_token_count: 240\n",
      "  candidates_token_count: 5\n",
      "  total_token_count: 245\n",
      "}\n",
      "\n",
      "response.candidates[0].content.args role: \"model\"\n",
      "parts {\n",
      "  function_call {\n",
      "    name: \"entity_linking\"\n",
      "    args {\n",
      "      fields {\n",
      "        key: \"category\"\n",
      "        value {\n",
      "          string_value: \"dba\"\n",
      "        }\n",
      "      }\n",
      "    }\n",
      "  }\n",
      "}\n",
      "\n",
      "filter_generator_output: {'category': 'dba'}\n",
      "response: {'category': 'dba'}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "from entity_linking import EntityLinking\n",
    "from foundation_models.chat_openai import AIModelType\n",
    "from schema import topic_schema\n",
    "current_model = AIModelType.MISTRAL_SMALL\n",
    "\n",
    "\n",
    "async def link_entities(message: str):\n",
    "    # generate function call schema\n",
    "    schema = topic_schema\n",
    "    \n",
    "    # gemerate function calling schema\n",
    "    llm_module = EntityLinking(schema=schema,model=current_model)\n",
    "    \n",
    "    print(\"current_model:\", current_model)\n",
    "    \n",
    "    if current_model == AIModelType.MISTRAL_LARGE or current_model == AIModelType.MISTRAL_MIXTRAL_8x22B or current_model == AIModelType.MISTRAL_SMALL:\n",
    "        filter_generator_output = llm_module.generate_sync(conversation=message)\n",
    "        return filter_generator_output\n",
    "    if current_model == AIModelType.GPT4_O or current_model == AIModelType.GPT4_TURBO or current_model == AIModelType.GPT4_O_MINI or current_model == AIModelType.GPT3 or current_model == AIModelType.GOOGLE_GEMINI_PRO or current_model == AIModelType.CLAUDE_OPUS or current_model == AIModelType.CLAUDE_SONNET:\n",
    "        filter_generator_output = await llm_module.generate_response_generic(conversation=message)\n",
    "    else:\n",
    "        filter_generator_output = await llm_module.generate_async(conversation=message)\n",
    "        \n",
    "    print(\"filter_generator_output:\", filter_generator_output)\n",
    "    \n",
    "    return filter_generator_output\n",
    "        \n",
    "    \n",
    "# test the function\n",
    "message = \"What is the difference when speaking about 'LOB' and 'binary' data? Is it the same, in terms of binary data being stored in a separate filegroup, or are there differences?\"\n",
    "response = await link_entities(message)\n",
    "print(\"response:\", response)\n",
    "\n",
    "# save to json file\n",
    "with open('./temp.json', 'w') as f:\n",
    "    json.dump(response, f, indent=4, ensure_ascii=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# df = pd.read_json(f\"prediction_mantis_{current_model}_v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over rows with iterrows()\n",
    "import asyncio\n",
    "import json\n",
    "\n",
    "tasks = []\n",
    "\n",
    "for index, row in df.iterrows():\n",
    "    # predicted_entities = row[2]\n",
    "\n",
    "    # if predicted_entities is not None and type(predicted_entities) == str:\n",
    "    #     print(\"Already predicted entities\")\n",
    "    #     continue\n",
    "\n",
    "    print(row)\n",
    "    query = row[0]\n",
    "    actual_category = row[1]\n",
    "    print(actual_category)\n",
    "    print(query)\n",
    "\n",
    "    async def recognize_and_return_index(\n",
    "        index, message: str\n",
    "    ):\n",
    "        recognized_filters = (\n",
    "            await link_entities(\n",
    "                message=message,\n",
    "            )\n",
    "        )\n",
    "        \n",
    "        \n",
    "        predicted_category = recognized_filters.get(\"category\")\n",
    "        \n",
    "        if predicted_category is None:\n",
    "            predicted_category = \"None\"\n",
    "        \n",
    "        print(predicted_category)\n",
    "\n",
    "        df.at[index, \"Prediction\"] = predicted_category\n",
    "        return index\n",
    "\n",
    "    print(index)\n",
    "    \n",
    "    try:\n",
    "        await recognize_and_return_index(\n",
    "        index=index,\n",
    "        message=query,\n",
    "    )\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        continue\n",
    "\n",
    "    # wait to avoid rate limiting\n",
    "    await asyncio.sleep(5)\n",
    "\n",
    " \n",
    "    # write to csv\n",
    "    df.to_json(f\"prediction_mantis_{current_model}_v1.json\", index=False, orient=\"records\")\n",
    "\n",
    "\n",
    "print(len(tasks))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "current_model = AIModelType.GOOGLE_GEMINI_PRO\n",
    "\n",
    "# import pandas as pd\n",
    "import json\n",
    "df = pd.read_json(f\"./mantis-category-prediction/prediction_mantis_{current_model}_v1.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "\n",
    "def classification_metrics(true_labels, predicted_labels):\n",
    "    \"\"\"\n",
    "    Calculate and print classification metrics.\n",
    "\n",
    "    Parameters:\n",
    "    - true_labels: List of true labels\n",
    "    - predicted_labels: List of predicted labels\n",
    "\n",
    "    Returns:\n",
    "    - metrics: Dictionary containing accuracy, precision, recall, and F1-score\n",
    "    \"\"\"\n",
    "    metrics = {\n",
    "        'accuracy': accuracy_score(true_labels, predicted_labels),\n",
    "        'precision': precision_score(true_labels, predicted_labels, average='weighted'),\n",
    "        'recall': recall_score(true_labels, predicted_labels, average='weighted'),\n",
    "        'f1_score': f1_score(true_labels, predicted_labels, average='weighted')\n",
    "    }\n",
    "\n",
    "    print(f\"Accuracy: {metrics['accuracy']:.2f}\")\n",
    "    print(f\"Precision: {metrics['precision']:.2f}\")\n",
    "    print(f\"Recall: {metrics['recall']:.2f}\")\n",
    "    print(f\"F1 Score: {metrics['f1_score']:.2f}\")\n",
    "\n",
    "    return metrics\n",
    "\n",
    "# Example usage\n",
    "true_labels = df[\"category\"].tolist()\n",
    "predicted_labels = df[\"Prediction\"].tolist()\n",
    "\n",
    "classification_metrics(true_labels, predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have lots of false negatives, as the model does not have all filter ids in schema for Function Call Model. We use custom filter ids for the experiment, to give them a fair chance."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "shopping-assistant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
